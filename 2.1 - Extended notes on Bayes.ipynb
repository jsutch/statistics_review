{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>\n",
       "  table {margin-left: 0 !important;}\n",
       "</style>\n"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%%html\n",
    "<style>\n",
    "  table {margin-left: 0 !important;}\n",
    "</style>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### True Positives and False Negatives\n",
    "\n",
    "You have a test for a disease. \n",
    "\n",
    "*Tests correctly working*\n",
    "- Disease is present and the test indicates the disease is present **True Positive**\n",
    "- Disease is absent and the test indicates the disease is absent **True Negative**\n",
    "\n",
    "*Tests failing*\n",
    "- Disease is present and the test indicates the disease is absent **False Negative**\n",
    "- Disease is absent and the test indicates the disease is present **False Positive**\n",
    "\n",
    "|Actual||Predicted|\n",
    "|---|---|---|\n",
    "||*Negative*|*Positive*|\n",
    "|*Negative*|True Negative|False Positive|\n",
    "|*Positive*|False Negative|True Positive|\n",
    "\n",
    "\n",
    "<img src=\"true_positive_false_negative.png\" width=\"300\" height=\"200\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Bayes Theorem \n",
    "\n",
    "**Bayes' Theorem**  describes the probability of an event, based on conditions that might be related to the event. Bayes Theorem allows us to use previously known information to asess likelihood of another related event.\n",
    "\n",
    "Bayes’s theorem is named after Reverend Thomas Bayes (1701?–1761 - an English statistician, philosopher and Presbyterian minister), who first used conditional probability to provide an algorithm (his Proposition 9) that uses evidence to calculate limits on an unknown parameter, published as An Essay towards solving a Problem in the Doctrine of Chances (1763).\n",
    "\n",
    "\n",
    "in *Conditional Probability* we see that $P(A|B) = \\frac{P(A \\cap B)}{P(B)}$ provided that $P(B) > 0$\n",
    "\n",
    "So $P(B | A) = \\frac{P(B\\cap A}{P(A)} = \\frac{P(A\\cap B)}{P(A)}$ = provided that P(A) > 0\n",
    "\n",
    "These are two different ways to write out the probability of B, given A occuring. \n",
    "\n",
    "Connecting those two conditional probability formulas gets Bayes Theorem $ P(A|B) = \\frac{P(B|A)P(A)}{P(B)} $\n",
    "\n",
    "- The probability that A, given B has occurred, is equal to the probability of B, given that A has occurred, multiplied by the probability of A, divided by the probability of B, if B is > 0. \n",
    "- This joins together both \"B if A has occurred\" and \"A if B has occurred\"\n",
    "- Bayes Theory is used to determine the probability of a *Parameter* given a certain event.\n",
    "\n",
    "Usually Bayes Theorem is displayed in one of two ways:\n",
    "\n",
    "$ P(A|B) = \\frac{P(B|A)P(A)}{P(B)} $ or $ P(A|B) = \\frac{P(B|A)P(A)}{P(B|A)P(A) + P(B|not A)P(not A)}$ given that $P(B) > 0$\n",
    "\n",
    "\n",
    " \n",
    "\n",
    "**Exercise 1:**  \n",
    "- A manufacturing company finds that 1 out of every 500 products are defective (or .002) or %2\n",
    "- The company buys a diagnostic tool that correctly identifies a defective part 99% of the time (not %100)\n",
    "- If the part is diagnosed as defective, what's the probability that it's really defective (true positives/false negatives)\n",
    "\n",
    "\n",
    "\n",
    "The Questions:\n",
    "\n",
    "If a part is diagnosed as defective what is the probability that it really is defective?\n",
    "\n",
    "- $ P(A)$ The probability of being defective\n",
    "- $ P(B)$ The probability of testing as defective\n",
    "- $ P(A|B)$ The probability of being defective if the tool indicated a defect - True Positive\n",
    "- $ P(B|A)$ The probability of the tool indicating a defect if the product is defective\n",
    "\n",
    "True Positive - $P(A|B)$\n",
    "True Negative - \n",
    "False Positive - $P(B|- A)$\n",
    "False Negative -\n",
    "\n",
    "- $ P(A|B)$ - ? we're solving for this\n",
    "- $ P(B|A)$ The accuracy rate of the diagnostic tool (.99) or 99%\n",
    "- $ P(A)$  The correct failure rate (.002) (1 / 500) or 2%\n",
    "- $ P(B)$ - ? we need to calculate this\n",
    "\n",
    " \n",
    "\n",
    "1. **First calculate $P(B)$**\n",
    "- P(B) - Probability of testing positive  = (P(true positive) + P(false positive))\n",
    "\n",
    "Calculate this with:  \n",
    "**P(B) = $P(B|A) * P(A) + P(B|- A) * P(-A)$**\n",
    "- The probability of a True Positive ($P(B)$) is the probability of Testing Defective given that it was actually defective ($P(A|B)$) \n",
    "- Multiplied by the probability of an actual defect ($P(A)$) \n",
    "- Add the probability that it tested defective but was not defective (False Positive) ($P(B|- A)$)\n",
    "- Multiplied by the probability that it was not defective ($P(A)$)\n",
    "\n",
    "\n",
    "- $ P(B|A)$ The accuracy rate of the diagnostic tool (.99) or 99%\n",
    "- $ P(A)$  The correct failure rate (.002) (1 / 500) or 2%\n",
    "\n",
    "Use the Complement Rule (the Probability of \"Not A\" is \"1 minus the probability of A occurring\")\n",
    "\n",
    "*Calculate $P(B|-A)$:*\n",
    "- $P(B|-A)  = 1 - P(B|A) = 1 - .99 =$ **.01**\n",
    "\n",
    "*Calculate $P(-A)$*:\n",
    "- $P(-A) = 1 - P(A) = 1 - .002 =$ **.998** - The probability of getting a false positive.\n",
    "\n",
    "*Calculating $P(B)$*:\n",
    "- $P(B) = $P(B|A) * P(A) + P(B|- A) * P(-A)$ = .99 * .002 + .01 * .998  = **.01196**\n",
    "\n",
    "- $ P(A|B)$ = ??\n",
    "- $ P(B|A)$ = .99\n",
    "- $ P(A)$ = .002\n",
    "- $ P(B)$ = .01196\n",
    "- $ P(-A)$ = .998\n",
    "- $ P(B|-A)$ = .01 \n",
    "\n",
    "*So, plugging in the numbers to calculate $P(A|B)$*:\n",
    "\n",
    "numerator:  \n",
    "In [3]: .99*.002  \n",
    "Out[3]: 0.00198  \n",
    "denominator:  \n",
    "In [2]: .99 * .002 + .01 * .998  \n",
    "Out[2]: 0.011960000000000002\n",
    "\n",
    "The expanded formula:\n",
    "- $P(A|B) = \\frac{P(B|A) * P(A)} {P(B|A) * P(A) + P(B|-A) * P(-A)}$\n",
    "\n",
    "with the real numbers:\n",
    "\n",
    "- $P(A|B) = \\frac{.99 * .002} {.99 * .002 + .01 * .998} = \\frac{0.00198}{0.01196} = .165$ or **%16.5** \n",
    "\n",
    "In [118]: 0.00198 / 0.011960000000000002  \n",
    "Out[118]: 0.16555183946488292\n",
    "\n",
    "\n",
    "So a positive test only has a **%16.5 chance of correctly identifying a defective part** \n",
    "- $ P(A|B)$ = .165 or **%16.5 True Positive Rate**\n",
    "- $ P(B|A)$ = .99 Probability of the test producing a True Positive (actual defect)\n",
    "- $ P(A)$ = .002  Probability of Being Defective\n",
    "- $ P(B)$ = .01196 Probability of Testing Defective\n",
    "\n",
    "\n",
    "**Exercise 2:** What if a second test on the same part comes up that also returns positive (shows a defect)?\n",
    "\n",
    "- Fill in the details from the first run through.\n",
    "- Because it's already gone through the diagnostic test the probability goes up.\n",
    "\n",
    "- $ P(A|B)$ = ??\n",
    "- $ P(B|A)$ = .99\n",
    "- $ P(A)$ = Changes from .002 to .165\n",
    "- $ P(B)$ = .01196\n",
    "- $ P(-A)$ = Changes from .998 to .835\n",
    "- $ P(B|-A)$ = .01 \n",
    "\n",
    "So:\n",
    "- $P(A|B) = \\frac{P(B|A) * P(A)} {P(B|A) * P(A) + P(B|-A) * P(-A)}$\n",
    "\n",
    "with the real numbers:\n",
    "\n",
    "numerator:  \n",
    "In [4]: .99*.165  \n",
    "Out[4]: 0.16335\n",
    "\n",
    "denominator:  \n",
    "In [6]: .99 * .165 + .01 * .835  \n",
    "Out[6]: 0.1717\n",
    "\n",
    "total:  \n",
    "In [7]: 0.16335 / 0.1717  \n",
    "Out[7]: 0.9513686662783926\n",
    "\n",
    "- $\\require{enclose} P(A|B) = \\frac{.99 *  \\enclose{horizontalstrike}{.002} .165} {.99 * \\enclose{horizontalstrike}{.002} .165 + .01 * \\enclose{horizontalstrike}{.998} .835} = \\frac{0.16335}{0.1717} = 0.9513$ or **%95**\n",
    "\n",
    "So the probability gets much higher and closer to $ P(B|A)$ - the diagnostic tool's error rate\n",
    "- $P(A|B) = \\frac{.99 * .165} {.99 * .165 + .01 * .835} = .951$ or **95.1% probability that the part is defective**\n",
    "\n",
    "\n",
    "\n",
    "**Exercise 2: Try it a third time**\n",
    "- $ P(A|B)$ = ??\n",
    "- $ P(B|A)$ = .99\n",
    "- $ P(A)$ = Changes from .165 to .951\n",
    "- $ P(B)$ = .01196\n",
    "- $ P(-A)$ = Changes from .998 to 0.049\n",
    "- $ P(B|-A)$ = .01 \n",
    "\n",
    "numerator: \n",
    "In [8]: .99*.951\n",
    "Out[8]: 0.9414899999999999\n",
    "\n",
    "$ P(-A)$  \n",
    "In [9]: 1 - .951\n",
    "Out[9]: 0.049000000000000044\n",
    "\n",
    "denominator:\n",
    "In [10]: .99 * .951 + .01 * .049\n",
    "Out[10]: 0.9419799999999999\n",
    "\n",
    "Outcome:\n",
    "In [11]: 0.9414899999999999 / 0.9419799999999999\n",
    "Out[11]: 0.9994798191044396\n",
    "\n",
    "- $ P(A|B)$ = **%99.94** chance that the the part is actually showing a defect\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Other Bayesian notes:\n",
    "\n",
    "- base rate neglect\n",
    "- remember your priors when analyzing\n",
    "- \n",
    "\n",
    "**base rate fallacy**\n",
    "The base rate fallacy, also called base rate neglect or base rate bias, is a fallacy. If presented with related base rate information (i.e. generic, general information) and specific information (information pertaining only to a certain case), the mind tends to ignore the former and focus on the latter. Base rate neglect is a specific form of the more general extension neglect. \n",
    "\n",
    "Base-rate neglect refers to the phenomenon whereby people ignore or undervalue that probability, typically in lieu of less informative, but more intuitively appealing information about an individual case (Kahneman & Tversky, 1973).\n",
    "\n",
    "also known as: neglecting base rates, base rate neglect, prosecutor's fallacy \n",
    "\n",
    "**prior probability distribution**\n",
    "In Bayesian statistical inference, a prior probability distribution, often simply called the prior, of an uncertain quantity is the probability distribution that would express one's beliefs about this quantity before some evidence is taken into account. For example, the prior could be the probability distribution representing the relative proportions of voters who will vote for a particular politician in a future election. The unknown quantity may be a parameter of the model or a latent variable rather than an observable variable. \n",
    "\n",
    "Bayes' theorem calculates the renormalized pointwise product of the prior and the likelihood function, to produce the posterior probability distribution, which is the conditional distribution of the uncertain quantity given the data.\n",
    "\n",
    "Similarly, the prior probability of a random event or an uncertain proposition is the unconditional probability that is assigned before any relevant evidence is taken into account. \n",
    "\n",
    "\n",
    "\n",
    "### Bayesian Odds\n",
    "\n",
    "Bayes' Rule can be expressed in terms of odds: \n",
    "Posterior Odds = Prior Odds * Likelihood Ratio\n",
    "\n",
    "\n",
    "Odds, a:b, and probability\n",
    "\n",
    "Odds are commonly written as the ratio of two numbers separated by a colon. For example, if P(A) = 2/3, the odds would be 2, but this would most likely be written as 2:1.\n",
    "\n",
    "The relation between odds, a:b, and probability, p is as follows: \n",
    "\n",
    "$ a:b=p:(1-p)$\n",
    "\n",
    "$p=\\frac{a}{a+b} $\n",
    "\n",
    "Suppose you have a box that has a 5% chance of containing a diamond. You also have a diamond detector that beeps half of the time if there is a diamond, and one fourth of the time if there is not. You wave the diamond detector over the box and it beeps.\n",
    "\n",
    "- The prior odds of the box containing a diamond are 1:19. (95% of 20 = 19 with 1 chance that it's empty)\n",
    "- The likelihood ratio of a beep is 1/2:1/4 = 2:1. \n",
    "- The posterior odds are 1:19 * 2:1 = 2:19. \n",
    "- This corresponds to about a probability of 2/21, which is about 0.095 or 9.5%. \n",
    "\n",
    "||Contains Diamond|Doesn't contain diamond|\n",
    "|---|---|---|\n",
    "|Prior Odds Ratio|1 |19 | \n",
    "|Likelihood Ratio|2|1|\n",
    "||__|__|\n",
    "|Posterior Odds Ratio|2|21|\n",
    "\n",
    "\n",
    "Bayesian Proportionality\n",
    "\n",
    " Bayesian Thinking allows us to keep account of priors and likelihood information to predict a posterior probability.\n",
    "\n",
    "Imagine a management consultation firm hires only two types of employees: IT and business consultants. You meet an employee who is very shy, but don't know their job role. \n",
    "\n",
    "If your guess is IT using only shyness as an attribute, then you have fallen for an inherent cognitive bias: Base Rate Neglect. Base Rate Neglect occurs when we do not take into account the underlying proportion of a group in the population. To answer the question we need to find out the proportion of IT consultant to Business consultants. In this case for every 1 IT person the firm hires 10 business consultants for a ratio of 10 to 1.\n",
    "\n",
    "Another assumption could be made about shyness as an attribute. It would be fair to assume shyness is more common in IT as compared to business consultants (geeks vs people persons). Let’s assume, 75% of IT professionals are in fact shy corresponding to about 15% of business consultants.\n",
    "\n",
    "Use the proportion of employees in the firm as the prior odds. Then use the shyness as an attribute as the Likelihood. \n",
    "\n",
    "The figure below demonstrates when we take a product of the two, we get posterior odds.\n",
    "\n",
    "<img src=\"bayesian_proportions.png\" width=\"400\" height=\"200\">\n",
    "\n",
    " Bayesian Thinking allows us to keep account of priors and likelihood information to predict a *Posterior probability*.\n",
    " \n",
    " \n",
    "||IT Consultant|Business Consultant|\n",
    "|---|---|---|\n",
    "|Prior Odds Ratio|1 |10 | \n",
    "|Likelihood Ratio|75|15|\n",
    "||__|__|\n",
    "|Posterior Odds Ratio|1|2|\n",
    " \n",
    " or 2 to 1.\n",
    " \n",
    "\n",
    "#### Principles of Bayesian Thinking\n",
    "**Rule 1 – Remember your priors!**\n",
    "\n",
    "As we saw earlier how easy it is to fall for the base rate neglect trap. The underlying proportion in the population is often times neglected and we as human beings have a tendency to just focus on just the attribute. Think of priors as the underlying or the background knowledge which is essentially an additional bit of information in addition to the likelihood. A product of the priors together with likelihood determines the posterior odds/probability.\n",
    "\n",
    "**Rule 2 – Question your existing belief**\n",
    "\n",
    "This is somewhat tricky and counter-intuitive to grasp but question your priors. Present yourself with a hypothesis what if your priors were irrelevant or even wrong? How will that affect your posterior probability? Would the new posterior probability be any different than the existing one if your priors are irrelevant or even wrong?\n",
    "\n",
    "**Rule 3 – Update incrementally**\n",
    "\n",
    "We live in a dynamic world where evidence and attributes are constantly shifting. While it is okay to believe in well-tested priors and likelihoods in the present moment. However, always question does my priors & likelihood still hold true today? In other words, update your beliefs incrementally as new information or evidence surfaces. A good example of this would be the shifting sentiments of the financial markets. What holds true today, may not tomorrow? Hence, the priors and likelihoods must also be incrementally updated.\n",
    "\n",
    "\n",
    "### Posterior probabilities of hypotheses and Bayes factors\n",
    "\n",
    "\n",
    "**Prior Odds**  \n",
    "$O[H_{1}:H_{2}] = \\frac {P(H_{1})}{P(H_{2}})$\n",
    "\n",
    "**Ratio of posterior probabilities and hypotheses**  \n",
    "\n",
    "$PO[H_{1}:H_{2}] = \\frac {P(H_{1}| data)}{P(H_{2}|data)}$\n",
    "- $PO[H_{1}:H_{2}]$ The probability of $H_{1}$ given data divided by the probability of  $H_{2}$ given data\n",
    "\n",
    "\n",
    "$PO[H_{1}:H_{2}] = \\frac {P(H_{1}| data)}{P(H_{2}|data)}$\n",
    "\n",
    "Posterior odds expanded:\n",
    "\n",
    "$PO[H_{1}:H_{2}] = \\frac {P(H_{1}| data)}{P(H_{2}|data)} = \\frac{(P(data|H1) * P(H1)/P(data)}{P(data|H2 * P(H2)/P(data)}$\n",
    "\n",
    "The probability of data in the numerator and denominator cancel out, leaving us with:\n",
    "\n",
    "$\\frac{(P(data|H1 * P(H1)}{P(data|H2 * P(H2)}$\n",
    "\n",
    "- reorganize that as the ratio of the data (given H1) and the data (given H2) multiplied by the ratio of the prior probabilities based on this hypothesis.\n",
    "\n",
    "$\\frac{(P(data|H1)}{P(data|H2} * \\frac{P(H1)}{* P(H2)}$\n",
    "\n",
    "The first half is called The Bayes Factor:\n",
    "\n",
    "$\\frac{(P(data|H1)}{P(data|H2}$\n",
    "\n",
    "and the second half is the Prior Odds:\n",
    "\n",
    "$\\frac{P(H1)}{* P(H2)}$\n",
    "\n",
    "In other words  Posterior Odds is the product of the Bayes Factor and the Prior Odds.\n",
    "\n",
    "- *Bayes Factor* quantifies the evidence of data arising from  Hypothesis-1 vs Hypothesis-2\n",
    "- in a *discrete case* this ist just the ratio of the likelihoods of the observed data under the two hypotheses.\n",
    "- in a *continuous case* it's the ratio of the *marginal likelihoods* $BF[H1:H2] = \\frac{\\int P(data | \\theta, H_{1}), d\\theta}{\\int P(data | \\theta, H_{2}), d\\theta} $\n",
    "\n",
    "HIV Testing With ELISA Example:\n",
    "*Hypotheses*:  \n",
    "$H_{1}$ - patient does not have HIV\n",
    "$H_{2}$ - patient does have HIV\n",
    "\n",
    "*Priors*:  \n",
    "$P(H_{1})$ - 0.99852  \n",
    "$P(H_{2})$ - 0.00148\n",
    "\n",
    "In [12]: 0.99852 / 0.00148  \n",
    "Out[12]: 674.6756756756756\n",
    "\n",
    "\n",
    "*Posteriors*:  \n",
    "$P(H_{1}|+)$ = .8788551  \n",
    "$P(H_{1}|+)$ = .1211449\n",
    "\n",
    "Posterior Odds:\n",
    "In [13]: .8788551 / .1211449  \n",
    "Out[13]: 7.254577782473715\n",
    "\n",
    "\n",
    "Interpreting the Bayes Factor:  \n",
    "Jefferys - 1961\n",
    "- if they Bayes Factor is between 1 and 3, the evidence against H2 isn't worthwhile\n",
    "- 3-20 the evidence is positive\n",
    "- 20-150 the evidence is strong\n",
    "- >150 - very strong\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## More Bayesian notes\n",
    "\n",
    "- ned says, 'philosophically, the differences are immense and have been appreciated for a long time. in practice, when you have enough data, the likelihood overwhelms the prior anyways, and the computational cost of the Bayesian approach is often not warranted (in the sense that the frequentist analysis will give you the same answer)'\n",
    "- ned says, 'it's when you don't have very much data that the Bayesian machinery is rather useful'\n",
    "\n",
    "\n",
    "\n",
    "**Bayesian inference** is a general approach to statistics which uses prior probabilities to answer questions like:\n",
    "- Has this happened before?\n",
    "- Is it likely, based on my knowledge of the situation, that it will happen?\n",
    "\n",
    "Prior probability is a probability distribution that summarizes established beliefs about an event before (i.e. prior to) new evidence is considered. When the new evidence is added, the new distribution is called posterior probability. The probabilities, which you can think of as degrees of belief, are called Bayesian probabilities. For some examples of Bayes probability, see:\n",
    "\n",
    "    Inverse Probability (which is another name for Bayes probability)\n",
    "    Bayes Theorem Problems (some step-by-step examples of using Bayes Theorem)\n",
    "\n",
    "\n",
    "\n",
    "**Frequentists vs Bayesians** \n",
    "\n",
    "The opposite of Bayesian statistics is frequentist statistics —the type of statistics you study in an elementary statistics class. In elementary statistics, you use rigid formulas and probabilities. Bayesian probabilities are a lot more flexible.\n",
    "\n",
    "**Bayesians** are subjective and uses a priori beliefs to define a prior probability distribution on the possible values of the unknown parameters.\n",
    "\n",
    "\n",
    "**Frequentists** see probability as something that has to do with a limiting frequency based on an observed proportion.\n",
    "- defines an event's probability as the limit of its relative frequency in many trials. \n",
    "- Probabilities can be found (in principle) by a repeatable objective process (and are thus ideally devoid of opinion).\n",
    "\n",
    "**Example using lottery balls**  \n",
    "If you're pulling balls from the the 'air popper' lottery ball system, what are the odds that the ball will be even?  \n",
    "We might say a priori \"1 in 2\", but you don't know if the balls are actually a fair sample of 1-100.\n",
    "\n",
    "- A frequentist will say: \"count them, then you'll know\"\n",
    "- A bayesian works on the idea that you have some prior commitments about the way the world is. In this example:\n",
    "'it's very unlikely that the government would permit a biased sample of balls in the lottery, therefore it's only going to be all the balls 1-100, represented only once and therefore it's a 1 in 2 chance.\n",
    "- Bayesianism works well in a constructed system like a lottery where prior assumptions are more meaningful.\n",
    "- in the example that there will be a new disease in a certain populations, then prior assumptions can get messy quickly. \n",
    "\n",
    "*\"If we aren't actually Bayesians in practice, we're pretty close to it\".*\n",
    "\n",
    " A good way to model quickly and infer things in complicated scenarios (satisficing?) \n",
    " \n",
    " \"if we're bayesians, we're unconcious bayesians\", because we're not perfectly rational.\n",
    " \n",
    " \"Humans are not good at reasoning in a probabilistic fashion. Statistics were invented to overcome that\"\n",
    " \n",
    " Is Bayesianism a \"description of what we do\" or a \"prescription for what we should do\"? \n",
    " \n",
    " \"With new observations you tend to refine your priors over time, so a bayesian will improve and refine over time\"\n",
    " \n",
    " \n",
    " \n",
    " \n",
    "\n",
    "<img src=\"frequentists_vs_bayesians.png\" width=\"400\" height=\"200\">\n",
    "\n",
    "\n",
    "**Bayesian Hypothesis Testing**\n",
    "\n",
    "- Traditional testing (the type you probably came across in elementary stats or AP stats) is called Non-Bayesian. It is how often an outcome happens over repeated runs of the experiment. It’s an objective view of whether an experiment is repeatable.\n",
    "- Bayesian hypothesis testing is a subjective view of the same thing. It takes into account how much faith you have in your results. In other words, would you wager money on the outcome of your experiment?\n",
    "\n",
    "**Differences Between Traditional and Bayesian Hypothesis Testing**\n",
    "\n",
    "- Traditional testing (Non Bayesian) requires you to repeat sampling over and over, while Bayesian testing does not. The main different between the two is in the first step of testing: stating a probability model. \n",
    "- In Bayesian testing you add prior knowledge to this step. It also requires use of a posterior probability, which is the conditional probability given to a random event after all the evidence is considered.\n",
    "\n",
    "  \n",
    "**Arguments for Bayesian Testing**  \n",
    "Many researchers think that it is a better alternative to traditional testing, because it:\n",
    "- Includes prior knowledge about the data.\n",
    "- Takes into account personal beliefs about the results.  \n",
    "**Arguments against**\n",
    "- Including prior data or knowledge isn’t justifiable.\n",
    "- It is difficult to calculate compared to non-Bayesian testing.\n",
    "\n",
    "\n",
    "\n",
    "**Here's five reasons why frequentists methods may be preferred:**\n",
    "- **Faster.** Given that Bayesian statistics often give nearly identical answers to frequentist answers (and when they don't, it's not 100% clear that Bayesian is always the way to go), the fact that frequentist statistics can be obtained often several orders of magnitude faster is a strong argument. Likewise, frequentist methods do not require as much memory to store the results. While these things may seem somewhat trivial, especially with smaller datasets, the fact that Bayesian and Frequentist typically agree in results (especially if you have lots of informative data) means that if you are going to care, you may start caring about the less important things. And of course, if you live in the big data world, these are not trivial at all.\n",
    "- **Non-parametric statistics.** I recognize that Bayesian statistics does have non-parametric statistics, but I would argue that the frequentist side of the field has some truly undeniably practical tools, such as the Empirical Distribution Function. No method in the world will ever replace the EDF, nor the Kaplan Meier curves, etc. (although clearly that's not to say those methods are the end of an analysis).\n",
    "- **Less diagnostics.** MCMC methods, the most common method for fitting Bayesian models, typically require more work by the user than their frequentist counter part. Usually, the diagnostic for an MLE estimate is so simple that any good algorithm implementation will do it automatically (although that's not to say every available implementation is good...). As such, frequentist algorithmic diagnostics is typically \"make sure there's no red text when fitting the model\". Given that all statisticians have limited bandwidth, this frees up more time to ask questions like \"is my data really approximately normal?\" or \"are these hazards really proportional?\", etc.\n",
    "- **Valid inference under model misspecification.** We've all heard that \"All models are wrong but some are useful\", but different areas of research take this more or less seriously. The Frequentist literature is full of methods for fixing up inference when the model is misspecified: bootstrap estimator, cross-validation, sandwich estimator (link also discusses general MLE inference under model misspecification), generalized estimation equations (GEE's), quasi-likelihood methods, etc. As far as I know, there is very little in the Bayesian literature about inference under model misspecification (although there's a lot of discussion of model checking, i.e., posterior predictive checks). I don't think this just by chance: evaluating how an estimator behaves over repeated trials does not require the estimator to be based on a \"true\" model, but using Bayes theorem does!\n",
    "- **Freedom from the prior (this is probably the most common reason for why people don't use Bayesian methods for everything).** The strength of the Bayesian standpoint is often touted as the use of priors. However, in all of the applied fields I have worked in, the idea of an informative prior in the analysis is not considered. Reading literature on how to elicit priors from non-statistical experts gives good reasoning for this; I've read papers that say things like (cruel straw-man like paraphrasing my own) \"Ask the researcher who hired you because they have trouble understanding statistics to give a range that they are 90% certain the effect size they have trouble imagining will be in. This range will typically be too narrow, so arbitrarily try to get them to widen it a little. Ask them if their belief looks like a gamma distribution. You will probably have to draw a gamma distribution for them, and show how it can have heavy tails if the shape parameter is small. This will also involve explaining what a PDF is to them.\"(note: I don't think even statisticians are really able to accurately say a priori whether they are 90% or 95% certain whether the effect size lies in a range, and this difference can have a substantial effect on the analysis!). Truth be told, I'm being quite unkind and there may be situations where eliciting a prior may be a little more straightforward. But you can see how this is a can of worms. Even if you switch to non-informative priors, it can still be a problem; when transforming parameters, what are easily mistaken for non-informative priors suddenly can be seen as very informative! Another example of this is that I've talked with several researchers who adamantly do not want to hear what another expert's interpretation of the data is because empirically, the other experts tend to be over confident. They'd rather just know what can be inferred from the other expert's data and then come to their own conclusion. I can't recall where I heard it, but somewhere I read the phrase \"if you're a Bayesian, you want everyone to be a Frequentist\". I interpret that to mean that theoretically, if you're a Bayesian and someone describes their analysis results, you should first try to remove the influence of their prior and then figure out what the impact would be if you had used your own. This little exercise would be simplified if they had given you a confidence interval rather than a credible interval!\n",
    "- Of course, if you abandon informative priors, there is still utility in Bayesian analyses. Personally, this where I believe their highest utility lies; there are some problems that are extremely hard to get any answer from in using MLE methods but can be solved quite easily with MCMC. But my view on this being Bayesian's highest utility is due to strong priors on my part, so take it with a grain of salt.\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "**More Bayesian concepts** \n",
    "\n",
    "- Approximate Bayesian Computation (ABC): This set of techniques starts with a set of known summary statistics. A second set of the same statistics is calculated from a variety of potential models, and the candidates are placed in an acceptance/rejection loop. ABC favors those candidates that more closely match the known summary statistics (Medhi, 2014).\n",
    "- Admissible decision rule: A decision rule is a guideline to help you support or reject a null hypothesis. Generally, speaking, a decision rule is “admissible” if it is better than the set of all other possible decision rules. It’s similar to the line of best fit in regression analysis: it’s not a perfect fit, but it’s “good enough.”\n",
    "- Bayesian efficiency: An efficient design requires you to input parameter values; In a Bayesian efficient model you have to take your “best guess” at what those parameters might be. Figuring out what is an efficient design (and what isn’t) by hand is only possible for very small designs, as it’s a computationally complex process (Hess & Daly, 2010).\n",
    "- Bayes’ theorem: see Bayes Theorem Problems\n",
    "- Bayes factor: the Bayes factor is a measure of relative likelihood between two hypotheses, or what Cornfield (1976) calls the “relative betting odds.” The factor ranges between zero and infinity, where values close to zero are evidence against the null hypothesis and evidence for the alternate hypothesis (Spiegelhalter, D. et. al, 2004).\n",
    "- Bayesian network: A directed acyclic graph that represents a set of variables and their associated dependencies.\n",
    "- Bayesian linear regression: treats regression coefficients and errors as random variables, instead of fixed unknowns. This tends to make the model more intuitive and flexible. However, the results are similar to simple linear regression if priors are uninformative and N is much greater than P (i.e. when the number of items is much greater than the number of prior distributions).\n",
    "- Bayesian estimator: Also called a Bayes action, the Bayes estimator is defined as a minimizer of Bayes risk. In more general terms, it’s a single number that summarizes information found in a prior distribution about a particular parameter.\n",
    "- Bayesian Information Criterion (also called the Schwarz criterion): given a set of models to choose from, you should choose the model with the lowest BIC.\n",
    "- Bernstein–von Mises theorem: This is the Bayesian equivalent of the asymptotic normality results in the asymptotic theory of maximum likelihood estimation (Ghosh & Ramamoorthi, 2006, p.33).\n",
    "- Conjugate prior: A conjugate prior has the same distribution as your posterior prior. For example, if you’re studying people’s weights, which are normally distributed, you can use a normal distribution of weights as your conjugate prior.\n",
    "- Credible interval: a range of values where an unobserved parameter falls with a certain subjective probability. It is the Bayesian equivalent of a confidence interval in frequentist statistics.\n",
    "- Cromwell’s rule: This simple rule states that you should not assign probabilities of 0 (an event will not happen) or 1 (an event will happen), except when you can demonstrate an event is logically true or false. For example, the event 5 + 5 will logically add up to 10, so you can apply a probability of 1 to it.\n",
    "- Empirical Bayes method: a technique where the prior distribution is estimated from actual data. This is unlike the usual Bayesian methods, where the prior distribution is fixed at the beginning of an experiment.\n",
    "- Hyperparameter: a parameter from the prior distribution that’s set before the experiment begins.\n",
    "- Likelihood function: A measurement of how well the data summarizes these parameters. See: What is the Likelihood function?\n",
    "- Maximum a posteriori estimation: An estimate of an unknown; It is equal to the mode of the posterior distribution.\n",
    "- Maximum entropy principle: This principle states that if you are estimating a probability distribution, you should select the distribution which gives you the maximum uncertainty (entropy).\n",
    "- Posterior probability: Posterior probability is the probability an event will happen after all evidence or background information has been taken into account. See: What is Posterior Probability?\n",
    "- Principle of indifference: states that if you have no reason to expect one event will happen over another, all events should be given the same probability.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## More Bayes examples\n",
    "\n",
    "\n",
    "*Example 1:*  \n",
    "\n",
    "\n",
    "\n",
    "$ P(A|B) = \\frac{P(B|A)P(A)}{P(B)} $\n",
    "\n",
    "- In a pain clinic 10% of patients are prescribed narcotic pain killers. \n",
    "- Overall 5% of the clinic's patients are addicted to narcotics (including pain killers and illegal substances)\n",
    "- Out of all the people prescribed pain pills 8% are addicts\n",
    "\n",
    "If a patient is an addict whatis the probability that they are prescribed pain pills.\n",
    "\n",
    "\n",
    "Step 1: Figure out what your event “A” is from the question. That information is in the italicized part of this particular question. The event that happens first (A) is being prescribed pain pills. That’s given as 10%.\n",
    "\n",
    "Step 2: Figure out what your event “B” is from the question. That information is also in the italicized part of this particular question. Event B is being an addict. That’s given as 5%.\n",
    "\n",
    "Step 3: Figure out what the probability of event B (Step 2) given event A (Step 1). In other words, find what (B|A) is. We want to know “Given that people are prescribed pain pills, what’s the probability they are an addict?” That is given in the question as 8%, or .8.\n",
    "\n",
    "Step 4: Insert your answers from Steps 1, 2 and 3 into the formula and solve.\n",
    "P(A|B) = P(B|A) * P(A) / P(B) = (0.08 * 0.1)/0.05 = 0.16\n",
    "\n",
    "\n",
    "- $ A$  Being prescribed pain pills (%10)\n",
    "- $ B$  Being an addict (%5)\n",
    "- $ P(B|A)$ given that someone is prescribed a pain pill what's the probability they are an addict 8%\n",
    "\n",
    "$ P(A|B) = \\frac{P(B|A)P(A)}{P(B)} $\n",
    "$ P(A|B) = \\frac{.08 * .10 }{.05} = \\frac{.008}{.05} = .16 $ or 16%\n",
    "\n",
    "P(A|B) = 16%\n",
    "\n",
    "*Example 2:*  \n",
    "\n",
    "You might be interested in finding out a patient’s probability of having liver disease if they are an alcoholic. “Being an alcoholic” is the test (kind of like a litmus test) for liver disease.\n",
    "\n",
    "- A could mean the event “Patient has liver disease.” Past data tells you that 10% of patients entering your clinic have liver disease. P(A) = 0.10.\n",
    "- B could mean the litmus test that “Patient is an alcoholic.” Five percent of the clinic’s patients are alcoholics. P(B) = 0.05.\n",
    "- You might also know that among those patients diagnosed with liver disease, 7% are alcoholics. This is your B|A: the probability that a patient is alcoholic, given that they have liver disease, is 7%.\n",
    "\n",
    "P(A) = .10\n",
    "P(B) = .05\n",
    "P(B|A) = .07\n",
    "\n",
    "$ P(A|B)= \\frac{.07 * .01}{.05} = \\frac{0.0007}{.05} = .14 $ or a 14% chance\n",
    "\n",
    "\n",
    "*Example 3*:  \n",
    "\n",
    "A slightly more complicated example involves a medical test (in this case, a genetic test):\n",
    "\n",
    "There are several forms of Bayes’ Theorem out there, and they are all equivalent (they are just written in slightly different ways). In this next equation, “X” is used in place of “B.” In addition, you’ll see some changes in the denominator. The proof of why we can rearrange the equation like this is beyond the scope of this article (otherwise it would be 5,000 words instead of 2,000!). However, if you come across a question involving medical tests, you’ll likely be using this alternative formula to find the answer:\n",
    "\n",
    "$Pr(A|X) = \\frac{Pr(X|A) * Pr(A)}{(Pr(X|A) * Pr(A)) + (Pr(X|\\sim A) * Pr(\\sim A))}$\n",
    "\n",
    "\n",
    "1% of people have a certain genetic defect.\n",
    "90% of tests for the gene detect the defect (true positives).\n",
    "9.6% of the tests are false positives.\n",
    "If a person gets a positive test result, what are the odds they actually have the genetic defect?\n",
    "\n",
    "The first step into solving Bayes’ theorem problems is to assign letters to events:\n",
    "\n",
    "- A = chance of having the faulty gene. That was given in the question as 1%. That also means the probability of not having the gene (~A) is 99%.\n",
    "- X = A positive test result.\n",
    "\n",
    "So:\n",
    "\n",
    "    P(A|X) = Probability of having the gene given a positive test result.\n",
    "    P(X|A) = Chance of a positive test result given that the person actually has the gene. That was given in the question as 90%.\n",
    "    p(X|~A) = Chance of a positive test if the person doesn’t have the gene. That was given in the question as 9.6%\n",
    "\n",
    "Now we have all of the information we need to put into the equation:  \n",
    "$ P(A|X) = (.9 * .01) / (.9 * .01 + .096 * .99) = 0.0865 (\\%8.65) $\n",
    "\n",
    "The probability of having the faulty gene on the test is 8.65%.\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## SciPy 2014 - \n",
    "\n",
    "https://www.youtube.com/watch?v=KhAUfqhLakw\n",
    "\n",
    "http://jakevdp.github.io/blog/2014/03/11/frequentism-and-bayesianism-a-practical-intro/\n",
    "\n",
    "frequentist/bayesian divide is a question of philosophy: what is the definition of probability?\n",
    "\n",
    "Frequentists: \n",
    "- frequencies of repeated events\n",
    "- variation of data and derived quantities of data in terms of fixed model parameters\n",
    "- models are fixed and data vary around them\n",
    "\n",
    "Bayesians: \n",
    "- related to our own certainty or uncertainty of events.\n",
    "- variation of beliefs about parameters in terms of fixed observed data\n",
    "- observed data is fixed and models vary around them\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.axes._subplots.AxesSubplot at 0x128e2c750>"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Simple example - Photon Flux in a star\n",
    "# Given the observed Flux values, what's the best estimate of the True Value?\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "np.random.seed(2) # for repeatability\n",
    "e = np.random.normal(30,3,50)\n",
    "F = np.random.normal(1000,e)\n",
    "\n",
    "w = .1 / e ** 2 \n",
    "best_guess = (w * F).sum() / w.sum()\n",
    "error = 1. / np.sqrt(w.sum())\n",
    "\n",
    "plt.axvline(x=985,  linestyle='-', lw=1)\n",
    "sns.scatterplot(F, e)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "## Simple example - Photon Flux in a star\n",
    "#### Given the observed Flux values, what's the best estimate of the True Value?\n",
    "\n",
    "Frequentist approach: *Maximum Likelihood*  \n",
    "model each observation F, drawn from a guassian with  width $e_{1}$ (errors)\n",
    "\n",
    "$P(D_{i}|F_{true}) = \\frac{1}{\\sqrt 2\\pi e^{2}} exp\\left [ \\frac{-(F_{i} - F_{true}^{2}}{2e^{2}_{i}} \\right ] $\n",
    "\n",
    "Building the likelihood - when you have multiple measurements you multiply the measurements together\n",
    "\n",
    "$ L(D|F_{true}) =  \\pi^{N}_{i=1}  P(D_{i}|F_{true})$\n",
    "\n",
    "Eventually comes up with a *Weighted Average* \n",
    "\n",
    "\n",
    "In Bayes it uses Posterior Probability:\n",
    "\n",
    "$P(F_{true}|D)$\n",
    "\n",
    "Compute our knowledge of F given the data, encoded as a probability. \n",
    "\n",
    "$ P(F_{true}|D) = \\frac{P(D|F_{true}) * P(F_{true})}{P(D)} $\n",
    "\n",
    "\n",
    "Explanation of terms:  \n",
    "- posterior \"is the thing you are interested in\": $P(F_{true}|D)$\n",
    "\n",
    "- likelihood: $ {P(D|F_{true})$\n",
    "\n",
    "- prior $ P(F_{true})$  \n",
    "\n",
    "- model evidence (data that I've observed):  $P(D)$  \n",
    "\n",
    "So:  \n",
    "$posterior = \\frac{likelihood * prior}{model evidence}$\n",
    "\n",
    "\n",
    "- posterior - brightness of a single star\n",
    "- likelihood - same thing as frequentist approach\n",
    "- prior - the controversial thing \"what's the probability distribution of this value we're interested in, *before* we take any data. Could be 'how bright are all the other stars in the sky?' (empirical prior). 'make all the fluxes in the scan arbitrarily equally probable' (non-informative prior). Non-informative is used a lot in practice and it's what frequentists complain about the most.\n",
    "- model evidence (denominator) - often simply a normalization. Ignored for this explanation.\n",
    "\n",
    "in this case return the same answer. \n",
    "\n",
    "Often the approaches return answers that are indistinguishable.\n",
    "\n",
    "\n",
    "Cases change when they get more complex. Some examples:\n",
    "\n",
    "- Handling of nuisance parameters (params that don't matter at the end but are important for analysis)\n",
    "- Interpretation of uncertainty (important)\n",
    "- Incorporation of prior information (bayesian analysis does this well)\n",
    "- Comparison and evaluation of models\n",
    "\n",
    "\n",
    "Example 1: Nuisance Parameters\n",
    "\n",
    "**Bayes Billiard Game**  \n",
    "\n",
    "Alice and Bob have a Gambling problem. Carol designs a game for them to play\n",
    "- The first ball divides the table into halves\n",
    "- Additional balls give a point to Alice or Bob\n",
    "- First person to 6 points wins\n",
    "\n",
    "<img src=\"bayes_billiards.png\" width=\"400\" height=\"200\">\n",
    "\n",
    "Then you can cover the table and make it a black box. We don't see what's happening, all we know are the results (Alice or Bob wins).\n",
    "\n",
    "Question:\n",
    "- in a certain game Alice has 5 points and Bob has 3 points. What are the odds that Bob is going to win?\n",
    "\n",
    "The unseen division of the table is a 'nuisance parameter'. It affects the problem and we have to solve it, but in the end we don't care. We want to know how much money Alice should bet.\n",
    "\n",
    "A Frequentist Approach:\n",
    "\n",
    "- p = the probability of Alice winning any roll (nuisance parameter)\n",
    "- Maximum likelihood estimate gives $\\hat{p} = 5/8$ - (5 out of the 8 balls land on Alice's side)\n",
    "- Probability of Bob winning (he needs 3 points) $P(B) = (1 - \\hat{p})^{3}$\n",
    "- P(B) = 0.053 Odds of 18 to 1 against.\n",
    "\n",
    "Bayesian Approach:\n",
    "\n",
    "**marginalization**  \n",
    "B - Bob Wins\n",
    "D - observed data\n",
    "\n",
    "$P(B|D) \\equiv \\int^{\\infty}_{-\\infty} P(B,p|D)dp$\n",
    "\n",
    "integrate over all possible p values\n",
    "\n",
    "*Some algebriac manipulation*\n",
    "\n",
    "$P(B|D) = \\int P(B|p,D)P(p|D)dp$\n",
    "\n",
    "$P(B|D) = \\int P(B|p,D)P(p|D)dp \\frac{P(D|p)P(p)}{P(D)}pd$\n",
    "\n",
    "Find P(B|D) = 0.091: odds of 10-1 against.\n",
    "\n",
    "When Bayesians Marginalize, they get rid of parameters they don't care about.\n",
    "\n",
    "Frequentist (18-1) vs Bayesian (10-1) \n",
    "\n",
    "- who's right? Who knows?\n",
    "- What's different?\n",
    "\n",
    " The bayesian approach allows nuisance parameters to vary through marginalization.\n",
    " The frequentist approach keeps the nuisance parameter fixed.\n",
    "\n",
    "Frequentists *can* allow the parameter to vary in certain ways.\n",
    "\n",
    "Varying nuisance parameters can add value.\n",
    "\n",
    "Conditioning vs Marginalization:\n",
    "\n",
    "- The frequentist is taking a slice of the joint probability and gets a narrow result (conditioning)\n",
    "\n",
    "\n",
    "<img src=\"conditioning_vs_marginalization.png\" width=\"400\" height=\"200\">\n",
    "\n",
    "\n",
    "\n",
    "- the bayesian is taking the whole probability, squishes it horizontally and gets a very wide result.\n",
    "\n",
    "<img src=\"conditioning_vs_margin-bayes.png\" width=\"400\" height=\"200\">\n",
    "\n",
    "\n",
    "Example 2: Uncertainties\n",
    "\n",
    "\"999 +/- 4\" - what does the \"+/-\" mean?\n",
    "\n",
    "\n",
    "**Confidence vs Credibility**\n",
    "\n",
    "Frequentists use the Confidence Interval:\n",
    "- if this experiment is repeated many times in 95% of these cases the computed confidence interval (varying) will contain the *true value* ( $\\theta$ ) (fixed)\n",
    "\n",
    "Baysians use the Credibile Region\n",
    "- Given our observed data there is a 95% probability tht the value of $\\theta$ (varying) *lies within the Credible Region* (fixed)\n",
    "\n",
    "The things that are *varying* and the things that are *fixed* are opposite. \n",
    "\n",
    "Baysian Problem called \"the inverse probability problem\" because you take the Frequentist problem and turn it on its head\n",
    "\n",
    "\n",
    "Uncertainties: Jaynes' Truncated Exponential\n",
    "\n",
    "Consider a model: $p(x|\\theta) = \\left \\{\n",
    "    \\begin{cases}\n",
    "      1 & \\text{exp(\\theta - )     ,     x > \\theta}\\\\\n",
    "      2 & \\text{0     ,     x < \\theta}\\\\\n",
    "    \\end{cases}\n",
    "    \\right \\}\n",
    "    $\n",
    "    \n",
    "(how to unbork the double left brace?)\n",
    "\n",
    "- We observe D = {10,12,15}\n",
    "- What are the 95% bounds on $\\theta$?\n",
    "\n",
    "\n",
    "(not finishing the example)\n",
    "\n",
    "\n",
    "<img src=\"confidence_vs_credibility.png\" width=\"400\" height=\"200\">\n",
    "\n",
    "\n",
    "In General a Frequentist 95% Confidence Interval is *Not* 95% likely to contain the true value. This common mistake is a Baysian Interpretation of a Frequentist Construct.\n",
    "\n",
    "\n",
    "Statistician: \"95% of of such confidence intervals in repeated experiments will contain the true value\"\n",
    "\n",
    "Scientist: \"so there's a 95% chance that the value is within this interval?\"\n",
    "\n",
    "Statistician: \"no, you see, parameters by definition can't vary, so referring to *chance* in this context is meaningless. The 95% refers to the interval itself.\"\n",
    "\n",
    "Scientist: \"so there's a 95% chance that the value is within this interval?\"\n",
    "\n",
    "Statistician: \"no, it's like this. the long term limiting frequency of the procedure for constructing this interval ensures that 95% of the resulting ensemble of intervals contains the value\"\n",
    "\n",
    "Scientist: \"Ok. So there's a 95% chance that the value is within this interval. Got it.\"\n",
    "\n",
    "\n",
    "*Conclusion*\n",
    "\n",
    "- Frequentism and Bayesianism fundamentally differ in their *definition* of probability\n",
    "- Results are similar for simple problems, but often differ for complex ones\n",
    "- Baysianism provides for a more natural handling of nuisance parameters and a more natural interpretation of errors\n",
    "- both paradigms are useful in the right situations, but be careful in interpreting the results (especially frequentist results) correctly.\n",
    "\n",
    "Bayesianism is more helpful for Communicating Scientific results to the public.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# An example of the photon count problem with a visualization\n",
    "\n",
    "# Generating some simple photon count data\n",
    "from scipy import stats\n",
    "np.random.seed(1)  # for repeatability\n",
    "\n",
    "F_true = 1000  # true flux, say number of photons measured in 1 second\n",
    "N = 50 # number of measurements\n",
    "F = stats.poisson(F_true).rvs(N)  # N measurements of the flux\n",
    "e = np.sqrt(F)  # errors on Poisson counts estimated via square root\n",
    "\n",
    "#plotting\n",
    "fig, ax = plt.subplots()\n",
    "ax.errorbar(F, np.arange(N), xerr=e, fmt='ok', ecolor='gray', alpha=0.5)\n",
    "ax.vlines([F_true], 0, N, linewidth=5, alpha=0.2)\n",
    "ax.set_xlabel(\"Flux\");ax.set_ylabel(\"measurement number\");\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Bayes' Theorem and Bayesian statistics from scratch - a beginner's guide.\n",
    "\n",
    "Maybe a clearer way to write:\n",
    "\n",
    "$ P(A|B) = \\frac{P(B|A)P(A)}{P(B)} $ \n",
    "\n",
    "is\n",
    "\n",
    "$ P(H|E) = \\frac{P(E|H)P(H)}{P(E)} $\n",
    "\n",
    "to show that:\n",
    "- H is the Hypothesis\n",
    "- E is the Evidence (Data)\n",
    "\n",
    "$ P(Hypothesis being True|Given Evidence) = \\frac{P(Evidence|Hypthesis)P(Hypthesis)}{P(Evidence)} $\n",
    "\n",
    "$\\frac{\\text \n",
    "Probability that the Evidence is true given the Hypothesis) * Probability of the Hypothesis being true}{Probability that the evidence is true} $ \n",
    "\n",
    "\n",
    "\n",
    "-------\n",
    "\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
